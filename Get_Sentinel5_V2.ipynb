{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_ENV=os.environ.get('PATH',None)\n",
    "if 'google' in GET_ENV:\n",
    "    GLOBAL_PATH='/content/drive/My Drive/Colab Notebooks/datas_sentinel5/'\n",
    "else:\n",
    "    GLOBAL_PATH='datas_sentinel5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour téléchargez les données, veuillez vous enregistrer sur le site ONDA: https://www.onda-dias.eu/cms/\n",
    "url='https://auth.creodias.eu/auth/realms/DIAS/protocol/openid-connect/token'\n",
    "user = '' #Email\n",
    "password = '' #Password\n",
    "\n",
    "# Paramètres de téléchargements\n",
    "_Update_API=False #True: Requête vers l'API de ONDA + enregistrement dans un CSV | False: Lecture des CSV enregistrés\n",
    "_Download=True #True: télécharge 1 fichier (i=x) de ONDA. \n",
    "_Tracking=True #True: write in tracking_files.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_poly(x):\n",
    "    poly=[[[elem.replace(\"((\",\"\")]] for elem in x[14:-3].split(\")),\")]\n",
    "    poly=[[j.split(\" \") for j in poly[i][0][0].split(\",\")] for i in range(0,len(poly))]\n",
    "    return [poly]\n",
    "\n",
    "#Distances\n",
    "def distance(origin, destination):\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6373 # km\n",
    "\n",
    "    dlat = np.radians(lat2-lat1)\n",
    "    dlon = np.radians(lon2-lon1)\n",
    "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) \\\n",
    "        * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = radius * c\n",
    "\n",
    "    return d\n",
    "\n",
    "def logger(msg=None, alert=None):\n",
    "    '''\n",
    "    Les alert='WARNING' or 'ERROR' g\n",
    "    to\n",
    "    logs_errors.txt\n",
    "    '''\n",
    "    #Get date\n",
    "    timer = '['+str(datetime.now())+']'\n",
    "    \n",
    "    #Check if logs files\n",
    "    if 'logs.txt' not in os.listdir(GLOBAL_PATH+'logs/'):\n",
    "        file = open(GLOBAL_PATH+'logs/'+\"logs.txt\", \"w\") \n",
    "        file.write(timer+' '+\"Logs initialisation\") \n",
    "        file.close() \n",
    "        \n",
    "    timer = timer + '['+str(alert)+']'\n",
    "\n",
    "    if alert in ['WARNING','ERROR','ERRORS']:\n",
    "        if 'logs_errors.txt' not in os.listdir(GLOBAL_PATH+'logs/'):\n",
    "            file = open(GLOBAL_PATH+'logs/'+\"logs_errors.txt\", \"w\") \n",
    "            file.write(timer+' '+\"Logs errors initialisation\") \n",
    "            file.close() \n",
    "\n",
    "        file = open(GLOBAL_PATH+'logs/'+\"logs_errors.txt\", \"a\") \n",
    "        file.write(\"\\n\"+timer+' '+str(msg)) \n",
    "        file.close() \n",
    "        \n",
    "    file = open(GLOBAL_PATH+'logs/'+\"logs.txt\", \"a\") \n",
    "    file.write(\"\\n\"+timer+' '+str(msg)) \n",
    "    file.close() \n",
    "\n",
    "\n",
    "def get_token(url, user, password, force=False):\n",
    "    if force == False:\n",
    "        if 'tokens' not in globals():\n",
    "            global tokens\n",
    "\n",
    "        now=pd.to_datetime('today')\n",
    "\n",
    "        if 'tokens' in globals():\n",
    "            exp_date = tokens.loc[tokens.expiration_date == max(tokens.expiration_date),'expiration_date']\n",
    "            if (exp_date[0] - now) > pd.Timedelta('1 minute'):\n",
    "                return tokens['access_token'][0]\n",
    "  \n",
    "    logger('Refreshing token')\n",
    "    payload = dict(client_id='CLOUDFERRO_PUBLIC', username=user, password=password, grant_type='password')\n",
    "    res = requests.post(url, data=payload)\n",
    "    res=json.loads(res.content.decode('utf-8'))\n",
    "    res['creation_date']=pd.to_datetime('today')\n",
    "    res['expiration_date']=pd.to_datetime('today')+pd.Timedelta(str(res['expires_in'])+' seconds')\n",
    "\n",
    "    tokens=pd.DataFrame.from_dict(res, orient='index').transpose()\n",
    "    return res['access_token']\n",
    "\n",
    "\n",
    "def get_startDate():\n",
    "    list_csv = pd.Series(os.listdir(GLOBAL_PATH+\"csv/\"))\n",
    "    list_csv=list_csv[list_csv.str.contains('.csv')]\n",
    "    if len(list_csv) == 0:\n",
    "        return ''\n",
    "  \n",
    "    max_date = pd.to_datetime(list_csv.str.slice(26,-4), format='%Y_%m_%d_%H_%M').dt.date.max()\n",
    "    max_date = max_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    return \"&startDate=\"+max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vérification existence dossiers\n",
    "ls=[\"\", \"archives\", \"csv\", \"temp\",'logs']\n",
    "for elem in ls:\n",
    "    if(os.path.isdir(GLOBAL_PATH+elem)==0):\n",
    "        try:\n",
    "            os.mkdir(GLOBAL_PATH+elem)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _Update_API:\n",
    "    req=0\n",
    "    q_limit=pd.to_datetime(\"today\")\n",
    "    logger('Update API')\n",
    "\n",
    "    while True:\n",
    "        url_api=\"http://finder.creodias.eu/resto/api/collections/Sentinel5P/search.json?_pretty=true&productType=L2__NO2___&maxRecords=2000&sortParam=published&sortOrder=ascending\"+get_startDate()\n",
    "\n",
    "        response=requests.get(url_api)\n",
    "        response=json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "        keys=['id','title','orbitNumber','processingLevel','productType','cloudCover','snowCover','startDate','completionDate','published','services']\n",
    "        infos=[[elem.get(key, elem['properties'].get(key)) for key in keys] for elem in response[\"features\"]]\n",
    "        infos=pd.DataFrame(infos,columns=keys)\n",
    "\n",
    "        infos=infos.join(pd.concat([pd.DataFrame.from_dict(elem, orient='index') for elem in infos['services']]).reset_index(drop=True)).drop(columns=['mimeType','services'])\n",
    "\n",
    "        date_min = pd.to_datetime(infos.published.min()).strftime('%Y_%m_%d_%H_%M')\n",
    "        date_max = pd.to_datetime(infos.published.max()).strftime('%Y_%m_%d_%H_%M')\n",
    "\n",
    "        if get_startDate()[-10:] == date_max[:10].replace('_','-'):\n",
    "            break\n",
    "\n",
    "        else: \n",
    "            infos.to_csv(GLOBAL_PATH+\"csv/infos_\"+date_min+\"_to_\"+date_max+\".csv\")\n",
    "            req+=1\n",
    "\n",
    "      #Sleeper : Max 60req/Min\n",
    "    if req > 59 :\n",
    "        waiting_time = 60 - (pd.to_datetime(\"today\") - q_limit).seconds\n",
    "        if waiting_time > 0 :\n",
    "            time.sleep(waiting_time)\n",
    "        \n",
    "        q_limit=pd.to_datetime(\"today\")\n",
    "        req=0\n",
    "        \n",
    "    elif (pd.to_datetime(\"today\") - q_limit).seconds > 60:\n",
    "        q_limit=pd.to_datetime(\"today\")\n",
    "        req=0\n",
    "\n",
    "logger('Update API successfull','SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Récup tous les fichiers infos\n",
    "logger('Loading infos file')\n",
    "\n",
    "infos=pd.DataFrame()\n",
    "\n",
    "list_infos = pd.Series(os.listdir(GLOBAL_PATH+\"csv/\"))\n",
    "list_infos=list_infos[list_infos.str.contains('.csv')]\n",
    "\n",
    "for csv in list_infos:\n",
    "    infos=pd.concat([infos, pd.read_csv(GLOBAL_PATH+'csv/'+csv, index_col=0)], axis=0)\n",
    "\n",
    "infos=infos.reset_index(drop=True)\n",
    "\n",
    "#Regarder les jours déjà téléchargés\n",
    "if 'tracking_files.csv' not in os.listdir(GLOBAL_PATH):\n",
    "    tracking_files=pd.DataFrame(columns=['date','number'])\n",
    "else:\n",
    "    tracking_files=pd.read_csv(GLOBAL_PATH+'tracking_files.csv')\n",
    "    tracking_files['date']=pd.to_datetime(tracking_files['date']).dt.date\n",
    "\n",
    "#Converti format date\n",
    "infos['date']=pd.to_datetime(infos.startDate)\n",
    "infos['date']=infos['date'].dt.date\n",
    "\n",
    "tab_days = infos.groupby('date')['id'].count().reset_index()\n",
    "tab_days=tab_days.merge(tracking_files, on='date', how='left').fillna(0)\n",
    "tab_days.columns=['date','available','owned']\n",
    "tab_days['to_dl']=tab_days['available']-tab_days['owned']\n",
    "\n",
    "#Days to run\n",
    "tab_days = tab_days[tab_days.to_dl > 0]\n",
    "\n",
    "logger('Infos file loaded','SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download day 2020-11-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b9dbfb4d384256ae7d323005921859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=177.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for day in tab_days:\n",
    "for day in tab_days.loc[:,'date']:\n",
    "    id_torun=infos[infos['date']==day].index\n",
    "    nfiles=len(id_torun)\n",
    "    \n",
    "    if _Download:\n",
    "        logger('Download day '+str(day))\n",
    "        print('Download day '+str(day))\n",
    "\n",
    "        for k in tqdm(id_torun):\n",
    "            try:\n",
    "                my_token = get_token(url, user, password)\n",
    "            except Exception as E:\n",
    "                logger(E, 'ERROR')\n",
    "\n",
    "            dl_url = infos.loc[k,\"url\"]+'?token='+my_token\n",
    "            r = requests.get(dl_url)\n",
    "            logger(str(r.status_code)+' for k='+str(k))\n",
    "\n",
    "            if r.status_code == 200 :\n",
    "                try:\n",
    "                    my_token = get_token(url, user, password)\n",
    "            \n",
    "                    with open(GLOBAL_PATH+'temp/'+infos.loc[k,\"id\"], 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "\n",
    "                    with zipfile.ZipFile(GLOBAL_PATH+'temp/'+infos.loc[k,\"id\"], 'r') as zip_ref:\n",
    "                        zip_ref.extractall(GLOBAL_PATH+'temp/')\n",
    "\n",
    "                    #Supp ZIP\n",
    "                    os.remove(GLOBAL_PATH+'temp/'+infos.loc[k,\"id\"])\n",
    "                    !trash-empty\n",
    "                except Exception as E:\n",
    "                    logger(E, 'ERROR')\n",
    "            else:\n",
    "                logger('Code '+str(r.status_code)+' when downloading', 'ERROR')\n",
    "\n",
    "    rootgrp=dict()\n",
    "    df_sat=dict()\n",
    "\n",
    "    logger('Cleaning part for day '+str(day))\n",
    "\n",
    "    for k in tqdm(id_torun):\n",
    "        logger('File '+str(k)+': cleaning...')\n",
    "\n",
    "        try:\n",
    "            rootgrp[k]=Dataset(GLOBAL_PATH+'temp/'+infos.loc[k,\"title\"]+\"/\"+infos.loc[k,\"title\"]+\".nc\", \"r\", format=\"NETCDF4\")\n",
    "        except:\n",
    "            logger('No such file in directory','ERROR')\n",
    "            continue\n",
    "\n",
    "        product_data=['longitude','latitude','nitrogendioxide_tropospheric_column','qa_value']\n",
    "        x1={elem:rootgrp[k].groups[\"PRODUCT\"].variables[elem][0].data.flatten() for elem in product_data}\n",
    "        df_sat[k]=pd.DataFrame.from_dict(x1)\n",
    "\n",
    "        support_data=['eastward_wind','northward_wind','surface_pressure','snow_ice_flag','surface_classification','surface_altitude','surface_altitude_precision']\n",
    "        x2=dict()\n",
    "        for elem in support_data:\n",
    "            try:\n",
    "                x2[elem]=rootgrp[k].groups[\"PRODUCT\"].groups['SUPPORT_DATA'].groups['INPUT_DATA'].variables[elem][0].data.flatten()\n",
    "            except:\n",
    "                x2[elem]=np.array([float('NaN')]*df_sat[k].shape[0])\n",
    "\n",
    "\n",
    "        x2=pd.DataFrame.from_dict(x2)\n",
    "\n",
    "        df_sat[k]=df_sat[k].join(x2)\n",
    "\n",
    "        #Close file\n",
    "        rootgrp[k].close()\n",
    "\n",
    "        #Del folder\n",
    "        shutil.rmtree(GLOBAL_PATH+'temp/'+infos.loc[k,\"title\"])\n",
    "\n",
    "    logger('Cleaning successfull','SUCCESS')\n",
    "    !trash-empty\n",
    "\n",
    "    logger('Add coordinates...')\n",
    "    #Get locations infos from latitude and longitude\n",
    "    coordinates=dict()\n",
    "    results=dict()\n",
    "    id_torun = list(df_sat.keys())\n",
    "\n",
    "    for k in tqdm(id_torun):\n",
    "        logger('File '+str(k)+': coordinates...')\n",
    "        coordinates[k] =list(zip(df_sat[k][\"latitude\"], df_sat[k][\"longitude\"]))\n",
    "\n",
    "        results[k] =rg.search(coordinates[k])\n",
    "        \n",
    "        \n",
    "        results[k]=pd.DataFrame.from_dict(results[k]).rename(columns={\"lat\":\"cc_lat\",\"lon\":\"cc_lon\",\"name\":\"cc_ville\",\"admin1\":\"cc_region\",\"admin2\":\"cc_departement\",\"cc\":\"cc_pays\"})\n",
    "        df_sat[k]=pd.concat([df_sat[k], results[k]], axis=1)\n",
    "\n",
    "        df_sat[k].cc_lon=df_sat[k].cc_lon.astype(float)\n",
    "        df_sat[k].cc_lat=df_sat[k].cc_lat.astype(float)\n",
    "\n",
    "        #Compute distance\n",
    "        df_sat[k][\"dist\"]=distance([df_sat[k].latitude, df_sat[k].longitude], [df_sat[k].cc_lat, df_sat[k].cc_lon])\n",
    "        #Drop if dist > 30\n",
    "        df_sat[k]=df_sat[k][df_sat[k].dist <= 30]\n",
    "\n",
    "        #Remove columns generated\n",
    "        df_sat[k]=df_sat[k].drop(['cc_lat', 'cc_lon', \"dist\"], axis=1)\n",
    "\n",
    "        #Save to csv\n",
    "        df_sat[k]['date']=pd.to_datetime(infos.loc[k,'startDate'])\n",
    "        \n",
    "        del(coordinates[k], results[k])\n",
    "\n",
    "    logger('Coordinates successfull','SUCCESS')\n",
    "\n",
    "    tosave=pd.concat([df_sat[elem] for elem in df_sat.keys()])\n",
    "    del(df_sat)\n",
    "\n",
    "    #Keep only QA > 0.5\n",
    "    tosave = tosave[tosave.qa_value >= 0.5]\n",
    "\n",
    "    cols_group = ['cc_pays','cc_departement','cc_region','cc_ville']\n",
    "\n",
    "    #Dict of aggregate\n",
    "    agg_dict=dict()\n",
    "    for elem in ['longitude','latitude','nitrogendioxide_tropospheric_column','qa_value','eastward_wind','northward_wind','surface_pressure','surface_altitude','surface_altitude_precision']:\n",
    "        agg_dict[elem]='mean'\n",
    "\n",
    "    for elem in ['nb_obs']:\n",
    "        agg_dict[elem]='sum'\n",
    "\n",
    "    if tosave.shape[0] > 0:\n",
    "\n",
    "        tosave['nb_obs']=1\n",
    "        #Get max values_counts()\n",
    "        snow_ice_flag_max=pd.crosstab([tosave[elem] for elem in cols_group], tosave['snow_ice_flag'])\n",
    "        snow_ice_flag_max=snow_ice_flag_max.idxmax(axis=1).values\n",
    "\n",
    "        surface_classification_max=pd.crosstab([tosave[elem] for elem in cols_group], tosave['surface_classification'])\n",
    "        surface_classification_max=surface_classification_max.idxmax(axis=1).values\n",
    "\n",
    "        #Groupby city\n",
    "        tosave=tosave.groupby(cols_group, as_index=False).agg(agg_dict)\n",
    "\n",
    "        tosave['snow_ice_flag_max']=snow_ice_flag_max\n",
    "        tosave['surface_classification_max']=surface_classification_max\n",
    "        tosave['Date']=str(day)\n",
    "\n",
    "    filename=\"archived_\"+str(day)+\".csv\"\n",
    "    #Save files\n",
    "    tosave.to_csv(GLOBAL_PATH+\"archives/\"+filename+'.gz', index=False, compression='gzip')\n",
    "\n",
    "    if _Tracking:\n",
    "        logger('Save date '+str(day)+' to csv')\n",
    "        tracking_files=tracking_files.append({'date': str(day), 'number':nfiles}, ignore_index=True)\n",
    "        tracking_files.to_csv(GLOBAL_PATH+'tracking_files.csv', index=False)\n",
    "\n",
    "    logger('WORK SUCCESSFULL WITHOUT ERRORS FOR DAY '+str(day),'SUCCESS')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
