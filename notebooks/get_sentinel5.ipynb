{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dvw4vOlDS4G8"
   },
   "source": [
    "# Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4ByTSKbS4G9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confidential import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6xgk5jcS4HA"
   },
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "path_drive=\"../\" #Directory if you use Google Colab and you want to save files\n",
    "path='../datas_sentinel5/'\n",
    "\n",
    "# Pour téléchargez les données, veuillez vous enregistrer sur le site ONDA: https://www.onda-dias.eu/cms/\n",
    "user = secrets.user #Email\n",
    "password = secrets.password #Password\n",
    "\n",
    "# Paramètres de téléchargements\n",
    "Param_API=False #True: Requête vers l'API de ONDA + enregistrement dans un CSV | False: Lecture des CSV enregistrés\n",
    "Param_Download=False #True: télécharge 1 fichier (i=x) de ONDA. \n",
    "Param_Tracking=False #True: write in tracking_files.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oG_2doWQS4HF"
   },
   "outputs": [],
   "source": [
    "def f_poly(x):\n",
    "    poly=[[[elem.replace(\"((\",\"\")]] for elem in x[14:-3].split(\")),\")]\n",
    "    poly=[[j.split(\" \") for j in poly[i][0][0].split(\",\")] for i in range(0,len(poly))]\n",
    "    return [poly]\n",
    "\n",
    "def timer():\n",
    "    return '['+datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+']'\n",
    "\n",
    "#Distances\n",
    "def distance(origin, destination):\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6373 # km\n",
    "\n",
    "    dlat = np.radians(lat2-lat1)\n",
    "    dlon = np.radians(lon2-lon1)\n",
    "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) \\\n",
    "        * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = radius * c\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrlYIoDBS4HP"
   },
   "outputs": [],
   "source": [
    "#Vérification existence dossiers\n",
    "ls=[\"../datas_sentinel5\", \"../datas_sentinel5/cleaned\", \"../datas_sentinel5/archives\", \"../datas_sentinel5/csv\"]\n",
    "for elem in ls:\n",
    "    if(os.path.isdir(elem)==0):\n",
    "        try:\n",
    "            os.mkdir(elem)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BU5GlPGgkc6U"
   },
   "outputs": [],
   "source": [
    "top=300\n",
    "\n",
    "#Updating and loaging files\n",
    "files=os.listdir(path_drive+\"datas_sentinel5/csv\")\n",
    "if len(files)!=0:\n",
    "    #Read more recent file\n",
    "    ls=[os.path.getmtime(path_drive+\"datas_sentinel5/csv/\"+elem) for elem in files]\n",
    "    infos=pd.read_csv(path_drive+\"datas_sentinel5/csv/\"+files[ls.index(max(ls))], index_col=0)\n",
    "    infos.creationDate=pd.to_datetime(infos.creationDate)\n",
    "    #Store max date in a variable\n",
    "    max_date=infos.creationDate.max()\n",
    "    \n",
    "    #If Timedelta > 1 day & Param_API is ON : We start API\n",
    "    if (pd.Timestamp.today(tz=\"UTC\")-max_date >= pd.Timedelta('1 days')) & Param_API:\n",
    "        url=\"https://catalogue.onda-dias.eu/dias-catalogue/Products?$search=%22name:S5P_OFFL_L2__NO2*%22&$top=\"+str(top)+\"&$orderby=creationDate%20desc&$skip=0&$format=json\"\n",
    "\n",
    "        response=requests.get(url)\n",
    "        response=json.loads(response.content.decode('utf-8'))\n",
    "        #Cleaning\n",
    "        keys=[\"id\",\"name\",\"creationDate\",\"beginPosition\",\"size\",\"downloadable\",\"offline\",\"footprint\"]\n",
    "        infos=[[elem.get(key) for key in keys] for elem in response[\"value\"]]\n",
    "        infos=pd.DataFrame(infos,columns=keys)\n",
    "        infos.creationDate=pd.to_datetime(infos.creationDate)\n",
    "        infos.beginPosition=pd.to_datetime(infos.beginPosition)\n",
    "        #Remove when creationDate < max_date\n",
    "        infos=infos[infos.creationDate >= max_date]\n",
    "        #Save as CSV\n",
    "        infos.to_csv(path_drive+\"datas_sentinel5/csv/infos_\"+infos.creationDate.dt.strftime('%Y_%m_%d_%H_%M')[len(infos.creationDate)-1]+\"_to_\"+infos.creationDate.dt.strftime('%Y_%m_%d_%H_%M')[0]+\".csv\")\n",
    "        del(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2967246,
     "status": "ok",
     "timestamp": 1588329585126,
     "user": {
      "displayName": "Thomas Pical",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GibbpzHVDtq1cGIXsM5M-rm3N7kHNUiTICgiL36jA=s64",
      "userId": "06664535501026898571"
     },
     "user_tz": -120
    },
    "id": "UEHCrXuTkeyl",
    "outputId": "8de3e70e-0b24-4d61-aa7f-bdd32b7b05d7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'../datas_sentinel5/S5P_OFFL_L2__NO2____20200420T201750_20200420T215920_13059_01_010302_20200422T125917.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9d26b395dc76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mrootgrp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mrootgrp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0minfos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"nc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"NETCDF4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;31m#Rework data as Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'../datas_sentinel5/S5P_OFFL_L2__NO2____20200420T201750_20200420T215920_13059_01_010302_20200422T125917.nc'"
     ]
    }
   ],
   "source": [
    "for full_loop in range(0,1):\n",
    "    global_start = time.time()\n",
    "    infos=pd.DataFrame()\n",
    "    ls=[i for i in os.listdir(path_drive+\"datas_sentinel5/csv\") if os.path.isfile(os.path.join(path_drive+\"datas_sentinel5/csv\",i)) and 'infos' in i]\n",
    "    for elem in ls:\n",
    "        df=pd.read_csv(path_drive+\"datas_sentinel5/csv/\"+elem, index_col=0)\n",
    "        df.creationDate=pd.to_datetime(df.creationDate)\n",
    "        df.beginPosition=pd.to_datetime(df.beginPosition)\n",
    "        infos=pd.concat([infos, df], axis=0)\n",
    "      \n",
    "    infos=infos.reset_index(drop=True)\n",
    "\n",
    "    #Group by day\n",
    "    calendar=infos.groupby(infos.beginPosition.dt.floor(\"D\")).sum()\n",
    "\n",
    "    #Add absent dates from the last date to today\n",
    "    while pd.Timestamp.today(tz=\"UTC\").floor(\"D\")-calendar.asfreq('D').index.max()>pd.Timedelta('1 days'):\n",
    "        calendar_add=pd.DataFrame([[calendar.index.max()+pd.Timedelta('1 days'),0,0,0]], columns=['beginPosition','size','downloadable','offline'])\n",
    "        calendar_add=calendar_add.set_index(\"beginPosition\")\n",
    "        calendar=pd.concat([calendar,calendar_add])\n",
    "\n",
    "    #Adds absent dates and replaces with 0\n",
    "    calendar=calendar.asfreq('D').fillna(0)\n",
    "\n",
    "    #Add day of week\n",
    "    calendar[\"dayofweek\"]=calendar.index.dayofweek.values\n",
    "\n",
    "  #As categorical\n",
    "    calendar[\"categorical\"]=0\n",
    "    calendar.loc[calendar.downloadable > 0,\"categorical\"]=1\n",
    "    calendar.loc[calendar.downloadable >= 12,\"categorical\"]=2\n",
    "\n",
    "    if os.path.isfile(path_drive+\"datas_sentinel5/tracking_files.csv\"):\n",
    "        tracking_files=pd.read_csv(path_drive+\"datas_sentinel5/tracking_files.csv\", index_col=0)\n",
    "        tracking_files.date=pd.to_datetime(tracking_files.date)\n",
    "        tracking_files=tracking_files.drop_duplicates()\n",
    "    else:\n",
    "        tracking_files=pd.DataFrame([[pd.Timestamp.today(tz=\"UTC\").floor(\"D\")-pd.Timedelta('31 days'),0]], columns=[\"date\",\"number\"])\n",
    "\n",
    "    #Group by day\n",
    "    tracking_files=tracking_files.groupby(tracking_files.date.dt.floor(\"D\")).sum()\n",
    "\n",
    "    #Add absent dates from the last date to today\n",
    "    tracking_files_add=pd.DataFrame([[calendar.index.min(),0],[calendar.index.max(),0]], columns=[\"date\",\"number\"]).set_index(\"date\")\n",
    "    tracking_files=pd.concat([tracking_files_add,tracking_files])\n",
    "    tracking_files=tracking_files.sort_index()\n",
    "    tracking_files=tracking_files.asfreq('D').fillna(0)\n",
    "\n",
    "    #Combine to calendar\n",
    "    calendar[\"number\"]=tracking_files[\"number\"]\n",
    "    calendar[\"new_available\"]=calendar.downloadable-calendar.number\n",
    "    #As categorical\n",
    "    calendar[\"categorical\"]=0\n",
    "    calendar.loc[calendar.new_available > 0,\"categorical\"]=1\n",
    "    calendar.loc[calendar.new_available >= 12,\"categorical\"]=2\n",
    "\n",
    "    if pd.isnull(calendar[calendar.categorical==2].index.max()):\n",
    "        sys.exit(\"Aucunes nouvelles données à télécharger\")\n",
    "\n",
    "    #Read files in Archives and Cleansed -> Remove thoses files from the list\n",
    "    remove_infos=os.listdir(path+\"cleaned\")\n",
    "    remove_infos=[elem[:-4]+\".zip\" for elem in remove_infos]\n",
    "    infos=infos[~infos.name.isin(remove_infos)]\n",
    "\n",
    "    #Create link to download\n",
    "    infos[\"urls\"]=\"https://catalogue.onda-dias.eu/dias-catalogue/Products(\"+infos.id+\")/$value\"\n",
    "    infos=infos.drop_duplicates()\n",
    "    infos=infos.sort_values(\"beginPosition\", ascending=False)\n",
    "    infos=infos[infos.beginPosition.dt.floor(\"D\")==calendar[calendar.categorical==2].index.max()]\n",
    "    infos=infos.drop_duplicates('name')\n",
    "    infos=infos.reset_index(drop=True)\n",
    "\n",
    "    i=list(range(0,infos.shape[0]))\n",
    "\n",
    "    if Param_Download==True:\n",
    "        #Check if file doesn't already exist\n",
    "        for k in i:\n",
    "            ls=[elem[:-3] for elem in os.listdir(path)]\n",
    "            if(infos.loc[k,\"name\"][:-4] in ls)==False:\n",
    "\n",
    "                print(timer()+'[INFO] Beginning download file '+ infos.name[k])\n",
    "                r = requests.get(infos.loc[k,\"urls\"], auth=(user, password))\n",
    "                print(timer()+'[INFO] Code '+str(r.status_code))\n",
    "                if r.status_code != 200:\n",
    "                    while r.status_code != 200:\n",
    "                        print('\\033[1;31;48m'+timer()+'[ERROR] Error '+str(r.status_code)+'. Retry in 1 minute')\n",
    "                        time.sleep(60)\n",
    "                        r = requests.get(infos.loc[k,\"urls\"], auth=(user, password))\n",
    "\n",
    "                with open(path+infos.loc[k,\"name\"], 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                print('\\033[0m'+timer()+'[INFO] Unzip file')\n",
    "                with zipfile.ZipFile(path+infos.loc[k,\"name\"], 'r') as zip_ref:\n",
    "                    zip_ref.extractall(path)\n",
    "                print(timer()+'[INFO] Delete zipfile')\n",
    "                os.remove(path+infos.loc[k,\"name\"])\n",
    "                print(timer()+'[Success] The file '+infos.name[k]+' has been downloaded and unzipped.')\n",
    "                #time.sleep(5)\n",
    "\n",
    "    if Param_Tracking==True:\n",
    "        #Save the number of files downloaded in the the tracking file\n",
    "        if os.path.isfile(path_drive+\"datas_sentinel5/tracking_files.csv\"):\n",
    "            temp=pd.read_csv(path_drive+\"datas_sentinel5/tracking_files.csv\", index_col=0)\n",
    "            temp_add=pd.DataFrame([[infos.beginPosition.max().floor(\"D\"),infos.shape[0]]], columns=[\"date\",\"number\"])\n",
    "            temp=pd.concat([temp, temp_add])\n",
    "            temp.to_csv(path_drive+\"datas_sentinel5/tracking_files.csv\")\n",
    "            del(temp, temp_add)\n",
    "        else:\n",
    "            pd.DataFrame([[infos.beginPosition.max().floor(\"D\"),infos.shape[0]]], columns=[\"date\",\"number\"]).to_csv(path_drive+\"datas_sentinel5/tracking_files.csv\")\n",
    "\n",
    "\n",
    "    #Save full data in dict\n",
    "    rootgrp=dict()\n",
    "    for k in i:\n",
    "        rootgrp[k]=Dataset(path+infos.loc[k,\"name\"][:-3]+\"nc\", \"r\", format=\"NETCDF4\")\n",
    "\n",
    "    #Rework data as Dataframe\n",
    "    df_sat=dict()\n",
    "    for k in i:\n",
    "        lon_x=rootgrp[k].groups[\"PRODUCT\"].variables[\"longitude\"][0].data.flatten()\n",
    "        lat_x=rootgrp[k].groups[\"PRODUCT\"].variables[\"latitude\"][0].data.flatten()\n",
    "        z_value=rootgrp[k].groups[\"PRODUCT\"].variables[\"nitrogendioxide_tropospheric_column\"][0].data.flatten()\n",
    "        qa_value=rootgrp[k].groups[\"PRODUCT\"].variables[\"qa_value\"][0].data.flatten()\n",
    "      \n",
    "        df_sat[k]=pd.DataFrame({'longitude': lon_x, 'latitude': lat_x, 'NO2':z_value,'quality':qa_value})\n",
    "\n",
    "    for k in i:\n",
    "        df_sat[k][\"date\"]=np.full((450,rootgrp[k].groups[\"PRODUCT\"].variables[\"time_utc\"].shape[1]), rootgrp[k].groups[\"PRODUCT\"].variables[\"time_utc\"][0]).flatten(\"F\")\n",
    "\n",
    "    #Get locations infos from latitude and longitude\n",
    "    coordinates=dict()\n",
    "    results=dict()\n",
    "    start_time = time.time()\n",
    "    for k in i:\n",
    "        coordinates[k] =list(zip(df_sat[k][\"latitude\"], df_sat[k][\"longitude\"]))\n",
    "        results[k] =rg.search(coordinates[k])\n",
    "      \n",
    "    print(timer()+'[INFO] Linked with countries in %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "    #Merge columns created\n",
    "    for k in i:\n",
    "        results[k]=pd.DataFrame.from_dict(results[k])\n",
    "        results[k]=results[k].rename(columns={\"lat\":\"cc_lat\",\"lon\":\"cc_lon\",\"name\":\"cc_ville\",\"admin1\":\"cc_region\",\"admin2\":\"cc_departement\",\"cc\":\"cc_pays\"})\n",
    "\n",
    "    for k in i:\n",
    "        df_sat[k]=pd.concat([df_sat[k], results[k]], axis=1)\n",
    "        #Convert to float\n",
    "        df_sat[k].cc_lon=df_sat[k].cc_lon.astype(float)\n",
    "        df_sat[k].cc_lat=df_sat[k].cc_lat.astype(float)\n",
    "\n",
    "    for k in i:\n",
    "        #Compute distances\n",
    "        df_sat[k][\"dist\"]=distance([df_sat[k].latitude, df_sat[k].longitude], [df_sat[k].cc_lat, df_sat[k].cc_lon])\n",
    "        #Keep country infos only if dist <= 30\n",
    "        df_sat[k].loc[df_sat[k].dist > 30, [\"cc_ville\",\"cc_region\", \"cc_region\", \"cc_departement\", \"cc_pays\"]]=float(\"NaN\")\n",
    "        #Remove columns generated\n",
    "        df_sat[k]=df_sat[k].drop(['cc_lat', 'cc_lon', \"dist\"], axis=1)\n",
    "\n",
    "    df_plot=pd.DataFrame()\n",
    "    for k in i:\n",
    "        df_sat[k].to_csv(path+\"cleaned/\"+infos.name[k][:-3]+\"csv\")\n",
    "        rootgrp[k].close()\n",
    "        #os.remove(path+infos.name[k][:-3]+\"nc\")\n",
    "\n",
    "    #Make some RAM space:\n",
    "    del(df_sat, coordinates, lat_x, lon_x, rootgrp, results, start_time)\n",
    "\n",
    "    #files_list=os.listdir(\"datas_sentinel5/cleaned\")\n",
    "    files_list=[i for i in os.listdir(path+\"cleaned\") if os.path.isfile(os.path.join(path+\"cleaned/\",i)) and 'S5P_OFFL_L2__NO2' in i]\n",
    "    i=list(range(0,len(files_list)))\n",
    "\n",
    "    #Row bind for plot\n",
    "    df_plot=pd.DataFrame()\n",
    "\n",
    "    for k in range(0,len(files_list)):\n",
    "        df_sat=pd.read_csv(path+\"cleaned/\"+files_list[k], index_col=0)\n",
    "        df_sat.date=pd.to_datetime(df_sat.date)\n",
    "        df_plot=pd.concat([df_plot, df_sat])\n",
    "        del(df_sat)\n",
    "        #Restrictions quality\n",
    "        df_plot=df_plot[df_plot[\"quality\"]>=0.75]\n",
    "\n",
    "    #It is mandatory to replace NaN by \"Undefined\" to not loose datas\n",
    "    df_plot=df_plot.fillna(\"Undefined\")\n",
    "    #Add Counter too\n",
    "    df_plot[\"counter\"]=1\n",
    "\n",
    "    #Time values\n",
    "    df_plot[\"hour_mean\"]=df_plot.date.dt.hour\n",
    "    df_plot[\"hour_std\"]=df_plot.date.dt.hour\n",
    "    df_plot[\"day_std\"]=df_plot.date.dt.day\n",
    "    df_plot[\"day_mean\"]=df_plot.date.dt.day\n",
    "    df_plot[\"dayofweek_std\"]=df_plot.date.dt.dayofweek\n",
    "    df_plot[\"dayofweek_mean\"]=df_plot.date.dt.dayofweek\n",
    "    df_plot[\"week\"]=df_plot.date.dt.week\n",
    "    df_plot[\"month\"]=df_plot.date.dt.month\n",
    "    df_plot[\"year\"]=df_plot.date.dt.year\n",
    "\n",
    "    #Aggregation\n",
    "    df_plot=df_plot.groupby([\"year\",\"month\",\"week\",\"cc_pays\",\"cc_departement\",\"cc_region\",\"cc_ville\"], as_index=False).agg({'longitude':'mean', 'latitude':'mean', 'NO2':'mean', 'quality':'mean', 'hour_mean':'mean', 'hour_std':'std', 'dayofweek_mean':'mean', 'dayofweek_std':'std', 'day_mean':'mean', 'day_std':'std','counter':'sum'})\n",
    "\n",
    "    from datetime import date\n",
    "    filename=\"archived_\"+str(df_plot.year.value_counts()[df_plot.year.value_counts()==df_plot.year.value_counts().max()].index[0])+\"_\"+str(df_plot.month.value_counts()[df_plot.month.value_counts()==df_plot.month.value_counts().max()].index[0])+\"_\"+str(int(df_plot.day_mean.value_counts()[df_plot.day_mean.value_counts()==df_plot.day_mean.value_counts().max()].index[0]))+\".csv\"\n",
    "    #Save files\n",
    "    df_plot.to_csv(path_drive+\"datas_sentinel5/archives/\"+filename)\n",
    "\n",
    "    #Cleaning\n",
    "    del(df_plot)\n",
    "    for k in range(0,len(files_list)):\n",
    "        os.remove(path+\"cleaned/\"+files_list[k])\n",
    "\n",
    "    print(timer()+'[SUCCESS] The day have been downloaded and cleaned in '+str((time.time() - global_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzpkJi57oUhZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUg--mm-S4H5"
   },
   "outputs": [],
   "source": [
    "i=list(range(0,infos.shape[0]))\n",
    "path='datas_sentinel5/'\n",
    "\n",
    "if Param_Download==True:\n",
    "    #Check if file doesn't already exist\n",
    "    for k in i:\n",
    "        ls=[elem[:-3] for elem in os.listdir(\"datas_sentinel5\")]\n",
    "        if(infos.loc[k,\"name\"][:-4] in ls)==False:\n",
    "\n",
    "            print(timer()+'[INFO] Beginning download file '+ infos.name[k])\n",
    "            r = requests.get(infos.loc[k,\"urls\"], auth=(user, password))\n",
    "            print(timer()+'[INFO] Code '+str(r.status_code))\n",
    "            if r.status_code != 200:\n",
    "                while r.status_code != 200:\n",
    "                    print('\\033[1;31;48m'+timer()+'[ERROR] Error '+str(r.status_code)+'. Retry in 1 minute')\n",
    "                    time.sleep(60)\n",
    "                    r = requests.get(infos.loc[k,\"urls\"], auth=(user, password))\n",
    "\n",
    "            with open(path+infos.loc[k,\"name\"], 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            print('\\033[0m'+timer()+'[INFO] Unzip file')\n",
    "            with zipfile.ZipFile(path+infos.loc[k,\"name\"], 'r') as zip_ref:\n",
    "                zip_ref.extractall(path)\n",
    "            print(timer()+'[INFO] Delete zipfile')\n",
    "            os.remove(path+infos.loc[k,\"name\"])\n",
    "            print(timer()+'[Success] The file '+infos.name[k]+' has been downloaded and unzipped.')\n",
    "            #time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_sentinel5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
